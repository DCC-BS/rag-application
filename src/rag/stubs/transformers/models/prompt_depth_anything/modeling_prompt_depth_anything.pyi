"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn

from ...file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings
from ...modeling_outputs import DepthEstimatorOutput
from ...modeling_utils import PreTrainedModel
from .configuration_prompt_depth_anything import PromptDepthAnythingConfig

_CONFIG_FOR_DOC = ...

class PromptDepthAnythingLayer(nn.Module):
    def __init__(self, config: PromptDepthAnythingConfig) -> None: ...
    def forward(self, prompt_depth: torch.Tensor) -> torch.Tensor: ...

class PromptDepthAnythingPreActResidualLayer(nn.Module):
    """
    ResidualConvUnit, pre-activate residual unit.

    Args:
        config (`[PromptDepthAnythingConfig]`):
            Model configuration class defining the model architecture.
    """
    def __init__(self, config) -> None: ...
    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor: ...

class PromptDepthAnythingFeatureFusionLayer(nn.Module):
    """Feature fusion layer, merges feature maps from different stages.

    Args:
        config (`[PromptDepthAnythingConfig]`):
            Model configuration class defining the model architecture.
    """
    def __init__(self, config: PromptDepthAnythingConfig) -> None: ...
    def forward(self, hidden_state, residual=..., size=..., prompt_depth=...):  # -> Any:
        ...

class PromptDepthAnythingFeatureFusionStage(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, size=..., prompt_depth=...):  # -> list[Any]:
        ...

class PromptDepthAnythingDepthEstimationHead(nn.Module):
    """
    Output head consisting of 3 convolutional layers. It progressively halves the feature dimension and upsamples
    the predictions to the input resolution after the first convolutional layer (details can be found in the DPT paper's
    supplementary material). The final activation function is either ReLU or Sigmoid, depending on the depth estimation
    type (relative or metric). For metric depth estimation, the output is scaled by the maximum depth used during pretraining.
    """
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: list[torch.Tensor], patch_height: int, patch_width: int) -> torch.Tensor: ...

class PromptDepthAnythingPreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """

    config_class = PromptDepthAnythingConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

class PromptDepthAnythingReassembleLayer(nn.Module):
    def __init__(self, config: PromptDepthAnythingConfig, channels: int, factor: int) -> None: ...
    def forward(self, hidden_state):  # -> Any:
        ...

class PromptDepthAnythingReassembleStage(nn.Module):
    """
    This class reassembles the hidden states of the backbone into image-like feature representations at various
    resolutions.

    This happens in 3 stages:
    1. Take the patch embeddings and reshape them to image-like feature representations.
    2. Project the channel dimension of the hidden states according to `config.neck_hidden_sizes`.
    3. Resizing the spatial dimensions (height, width).

    Args:
        config (`[PromptDepthAnythingConfig]`):
            Model configuration class defining the model architecture.
    """
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: list[torch.Tensor], patch_height=..., patch_width=...) -> list[torch.Tensor]:
        """
        Args:
            hidden_states (`List[torch.FloatTensor]`, each of shape `(batch_size, sequence_length + 1, hidden_size)`):
                List of hidden states from the backbone.
        """
        ...

class PromptDepthAnythingNeck(nn.Module):
    """
    PromptDepthAnythingNeck. A neck is a module that is normally used between the backbone and the head. It takes a list of tensors as
    input and produces another list of tensors as output. For PromptDepthAnything, it includes 2 stages:

    * PromptDepthAnythingReassembleStage
    * PromptDepthAnythingFeatureFusionStage.

    Args:
        config (dict): config dict.
    """
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states: list[torch.Tensor],
        patch_height: int | None = ...,
        patch_width: int | None = ...,
        prompt_depth: torch.Tensor | None = ...,
    ) -> list[torch.Tensor]:
        """
        Args:
            hidden_states (`List[torch.FloatTensor]`, each of shape `(batch_size, sequence_length, hidden_size)` or `(batch_size, hidden_size, height, width)`):
                List of hidden states from the backbone.
        """
        ...

PROMPT_DEPTH_ANYTHING_START_DOCSTRING = ...
PROMPT_DEPTH_ANYTHING_INPUTS_DOCSTRING = ...

@add_start_docstrings(
    """
    Prompt Depth Anything Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.
    """,
    PROMPT_DEPTH_ANYTHING_START_DOCSTRING,
)
class PromptDepthAnythingForDepthEstimation(PromptDepthAnythingPreTrainedModel):
    _no_split_modules = ...
    def __init__(self, config) -> None: ...
    @add_start_docstrings_to_model_forward(PROMPT_DEPTH_ANYTHING_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=DepthEstimatorOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        prompt_depth: torch.FloatTensor | None = ...,
        labels: torch.LongTensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple[torch.Tensor] | DepthEstimatorOutput:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):
            Ground truth depth estimation maps for computing the loss.

        Returns:

        Examples:

        ```python
        >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
        >>> import torch
        >>> import numpy as np
        >>> from PIL import Image
        >>> import requests

        >>> url = "https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/image.jpg?raw=true"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> image_processor = AutoImageProcessor.from_pretrained("depth-anything/prompt-depth-anything-vits-hf")
        >>> model = AutoModelForDepthEstimation.from_pretrained("depth-anything/prompt-depth-anything-vits-hf")

        >>> prompt_depth_url = "https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/arkit_depth.png?raw=true"
        >>> prompt_depth = Image.open(requests.get(prompt_depth_url, stream=True).raw)

        >>> # prepare image for the model
        >>> inputs = image_processor(images=image, return_tensors="pt", prompt_depth=prompt_depth)

        >>> with torch.no_grad():
        ...     outputs = model(**inputs)

        >>> # interpolate to original size
        >>> post_processed_output = image_processor.post_process_depth_estimation(
        ...     outputs,
        ...     target_sizes=[(image.height, image.width)],
        ... )

        >>> # visualize the prediction
        >>> predicted_depth = post_processed_output[0]["predicted_depth"]
        >>> depth = predicted_depth * 1000.
        >>> depth = depth.detach().cpu().numpy()
        >>> depth = Image.fromarray(depth.astype("uint16")) # mm
        ```"""
        ...

__all__ = ["PromptDepthAnythingForDepthEstimation", "PromptDepthAnythingPreTrainedModel"]
